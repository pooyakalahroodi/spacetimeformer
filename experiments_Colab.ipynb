{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pooyakalahroodi/spaceimeformer/blob/main/spaceimeformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpl0-g43NRUp"
      },
      "source": [
        "# How to install Spacetimeformer Multivariate Forecasting packages on google Colab\n",
        "## step by step \n",
        "\n",
        "> This repository was written and tested for python 3.8 and pytorch 1.11.0. Note that the training process depends on specific (now outdated) versions of pytorch lightning and torchmetrics.\n",
        "\n",
        "<br> git clone https://github.com/QData/spacetimeformer.git\n",
        "<br> cd spacetimeformer\n",
        "<br> conda create -n spacetimeformer python==3.8\n",
        "<br> conda activate spacetimeformer\n",
        "<br> pip install -r requirements.txt\n",
        "<br> pip install -e .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gFPa6inPri2"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5tEfugTQvKK"
      },
      "source": [
        "Before running next block, insert the file requirements_customized into the spacetimeformer folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQnZG7G7h4LZ",
        "outputId": "a3e469f8-daae-4f52-88c4-9dbfbf4e74e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spacetimeformer'...\n",
            "remote: Enumerating objects: 641, done.\u001b[K\n",
            "remote: Counting objects: 100% (203/203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 641 (delta 97), reused 136 (delta 55), pack-reused 438\u001b[K\n",
            "Receiving objects: 100% (641/641), 16.21 MiB | 22.76 MiB/s, done.\n",
            "Resolving deltas: 100% (317/317), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/QData/spacetimeformer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GsMRk54qPF8p",
        "outputId": "5bf375cf-9bc2-4d27-b2f3-d81f5015d5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pystan\n",
            "  Downloading pystan-3.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting aiohttp<4.0,>=3.6 (from pystan)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clikit<0.7,>=0.6 (from pystan)\n",
            "  Downloading clikit-0.6.2-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpstan<4.11,>=4.10 (from pystan)\n",
            "  Downloading httpstan-4.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (44.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19 in /usr/local/lib/python3.10/dist-packages (from pystan) (1.22.4)\n",
            "Collecting pysimdjson<6.0.0,>=5.0.2 (from pystan)\n",
            "  Downloading pysimdjson-5.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pystan) (67.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0,>=3.6->pystan)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0,>=3.6->pystan)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0,>=3.6->pystan)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0,>=3.6->pystan)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0,>=3.6->pystan)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting crashtest<0.4.0,>=0.3.0 (from clikit<0.7,>=0.6->pystan)\n",
            "  Downloading crashtest-0.3.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting pastel<0.3.0,>=0.2.0 (from clikit<0.7,>=0.6->pystan)\n",
            "  Downloading pastel-0.2.1-py2.py3-none-any.whl (6.0 kB)\n",
            "Collecting pylev<2.0,>=1.3 (from clikit<0.7,>=0.6->pystan)\n",
            "  Downloading pylev-1.4.0-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: appdirs<2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from httpstan<4.11,>=4.10->pystan) (1.4.4)\n",
            "Collecting marshmallow<4.0,>=3.10 (from httpstan<4.11,>=4.10->pystan)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webargs<9.0,>=8.0 (from httpstan<4.11,>=4.10->pystan)\n",
            "  Downloading webargs-8.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0,>=3.10->httpstan<4.11,>=4.10->pystan) (23.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.6->pystan) (3.4)\n",
            "Installing collected packages: pylev, pysimdjson, pastel, multidict, marshmallow, frozenlist, crashtest, async-timeout, yarl, webargs, clikit, aiosignal, aiohttp, httpstan, pystan\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 clikit-0.6.2 crashtest-0.3.1 frozenlist-1.3.3 httpstan-4.10.0 marshmallow-3.19.0 multidict-6.0.4 pastel-0.2.1 pylev-1.4.0 pysimdjson-5.0.2 pystan-3.7.0 webargs-8.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu102\n",
            "Collecting torch==1.11.0+cu102\n",
            "  Downloading https://download.pytorch.org/whl/cu102/torch-1.11.0%2Bcu102-cp310-cp310-linux_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu102\n",
            "  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.12.0%2Bcu102-cp310-cp310-linux_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading https://download.pytorch.org/whl/cu102/torchaudio-0.11.0%2Bcu102-cp310-cp310-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu102) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu102) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu102) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu102) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu102) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu102) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu102) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu102) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0+cu102 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0+cu102 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0+cu102 torchaudio-0.11.0+cu102 torchvision-0.12.0+cu102\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.29.34)\n",
            "Collecting cmdstanpy==0.9.68 (from -r requirements.txt (line 2))\n",
            "  Downloading cmdstanpy-0.9.68-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.8.2)\n",
            "Collecting performer-pytorch (from -r requirements.txt (line 8))\n",
            "  Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.65.0)\n",
            "Collecting nystrom-attention (from -r requirements.txt (line 10))\n",
            "  Downloading nystrom_attention-0.0.11-py3-none-any.whl (4.5 kB)\n",
            "Collecting pytorch-lightning==1.6 (from -r requirements.txt (line 11))\n",
            "  Downloading pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.1/582.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting netCDF4 (from -r requirements.txt (line 12))\n",
            "  Downloading netCDF4-1.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
            "Collecting omegaconf (from -r requirements.txt (line 14))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.12.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (4.7.0.72)\n",
            "Collecting wandb (from -r requirements.txt (line 17))\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from -r requirements.txt (line 18))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (4.0.0)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (3.3.0)\n",
            "Collecting torchmetrics==0.5.1 (from -r requirements.txt (line 21))\n",
            "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.0/283.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ujson (from cmdstanpy==0.9.68->-r requirements.txt (line 2))\n",
            "  Downloading ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.11.0+cu102)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6->-r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6->-r requirements.txt (line 11)) (2023.4.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6->-r requirements.txt (line 11)) (2.12.2)\n",
            "Collecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch-lightning==1.6->-r requirements.txt (line 11))\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6->-r requirements.txt (line 11)) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.4->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->-r requirements.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->-r requirements.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->-r requirements.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->-r requirements.txt (line 6)) (0.5.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->-r requirements.txt (line 7)) (1.16.0)\n",
            "Collecting local-attention>=1.1.1 (from performer-pytorch->-r requirements.txt (line 8))\n",
            "  Downloading local_attention-1.8.6-py3-none-any.whl (8.1 kB)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from performer-pytorch->-r requirements.txt (line 8))\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cftime (from netCDF4->-r requirements.txt (line 12))\n",
            "  Downloading cftime-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (3.1.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 14))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 17)) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 17))\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 17)) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 17)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 17))\n",
            "  Downloading sentry_sdk-1.24.0-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 17))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->-r requirements.txt (line 17))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 17))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 17)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 17)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 17)) (3.20.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (3.8.4)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 17))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 17)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 17)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 17)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 17)) (3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (0.40.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.3.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 17))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6->-r requirements.txt (line 11)) (3.2.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, axial-positional-embedding, pathtools\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=0825c19bc9e248b64a2b6aa53ff9563d4dba17b5da259f72f0398bc03393156c\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2882 sha256=4bfa98f386ad9e5482dcf72e8b699bd8384bb52878ecc120d4b4f9970e4296ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=6867d6d170d0f24f32330fdcc2fa55e5ab6eec05d356bfcc21eab913d2ba6f7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built antlr4-python3-runtime axial-positional-embedding pathtools\n",
            "Installing collected packages: pathtools, antlr4-python3-runtime, ujson, smmap, setproctitle, sentry-sdk, pyDeprecate, omegaconf, einops, docker-pycreds, cftime, torchmetrics, nystrom-attention, netCDF4, local-attention, gitdb, axial-positional-embedding, performer-pytorch, GitPython, cmdstanpy, wandb, pytorch-lightning\n",
            "  Attempting uninstall: cmdstanpy\n",
            "    Found existing installation: cmdstanpy 1.1.0\n",
            "    Uninstalling cmdstanpy-1.1.0:\n",
            "      Successfully uninstalled cmdstanpy-1.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "prophet 1.1.3 requires cmdstanpy>=1.0.4, but you have cmdstanpy 0.9.68 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 antlr4-python3-runtime-4.9.3 axial-positional-embedding-0.2.1 cftime-1.6.2 cmdstanpy-0.9.68 docker-pycreds-0.4.0 einops-0.6.1 gitdb-4.0.10 local-attention-1.8.6 netCDF4-1.6.3 nystrom-attention-0.0.11 omegaconf-2.3.0 pathtools-0.1.2 performer-pytorch-1.1.4 pyDeprecate-0.3.2 pytorch-lightning-1.6.0 sentry-sdk-1.24.0 setproctitle-1.3.2 smmap-5.0.0 torchmetrics-0.5.1 ujson-5.7.0 wandb-0.15.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.12.0\n",
            "  Downloading torchtext-0.12.0-cp310-cp310-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (2.27.1)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.11.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchtext==0.12.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.4)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed torchtext-0.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata==0.3.0\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.27.1)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (1.11.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchdata==0.3.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (3.4)\n",
            "Installing collected packages: torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "Successfully installed torchdata-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.24.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/spacetimeformer\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: spacetimeformer\n",
            "  Running setup.py develop for spacetimeformer\n",
            "Successfully installed spacetimeformer-1.5.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(os.path.dirname(\"//content/spacetimeformer/\"))\n",
        "!pip install pystan\n",
        "!pip install torch==1.11.0+cu102 torchvision==0.12.0+cu102 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu102\n",
        "!pip install -r requirements.txt\n",
        "!pip install torchtext==0.12.0\n",
        "!pip install torchdata==0.3.0\t\n",
        "!pip3 install wandb\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZC5y-8adOkS"
      },
      "source": [
        "##Note:\n",
        "* pystan version mentioned in the requirements file is not working. \n",
        "* you switch rom Runtime/Run time type to GPU in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-xDF9Xht_HB",
        "outputId": "7ddc5fef-5cc3-4c03-ba3a-ed5d5259b044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "\n",
        "wandb.login(key=\"****\")\n",
        "\n",
        "os.environ['STF_WANDB_ACCT'] = 'pooyakalahroodi'\n",
        "os.environ['STF_WANDB_PROJ'] = 'spacetimeformer'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVYoTrfYJNrZ",
        "outputId": "d8b45975-bb83-4beb-b31c-10aa3f9391d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/spacetimeformer/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python train.py spacetimeformer solar_energy -- --context_points 168 --target_points 24 --d_model 20 --d_ff 20 --enc_layers 5 --dec_layers 5 --l2_coeff 1e-3 --dropout_ff .2 --dropout_emb .1 --d_qk 10 --d_v 10 --n_heads 3 --run_name spatiotemporal_al_solar --batch_size 8 --class_loss_imp 0 --initial_downsample_convs 1 --decay_factor .8 --warmup_steps 1000 --gpus 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1yYixzEQ8K6"
      },
      "source": [
        "max_epochs is set to 2 in the pl.Ttrainer(gpus=args.gpus,\n",
        "        callbacks=callbacks,\n",
        "        logger=logger if args.wandb else None,\n",
        "        accelerator=\"dp\",\n",
        "        gradient_clip_val=args.grad_clip_norm,\n",
        "        gradient_clip_algorithm=\"norm\",\n",
        "        overfit_batches=20 if args.debug else 0,\n",
        "        accumulate_grad_batches=args.accumulate,\n",
        "        sync_batchnorm=True,\n",
        "        limit_val_batches=args.limit_val_batches,\n",
        "        max_epochs=2,\n",
        "        **val_control,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvjxMjVMykTq"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(\"//content/spacetimeformer/\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP9MADxdKshK",
        "outputId": "fb26dd21-b47e-4b81-e044-9eb748a77ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpooyakalahroodi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./data/STF_LOG_DIR/wandb/run-20230522_125541-6olxuf56\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdutiful-morning-10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer/runs/6olxuf56\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "Forecaster\n",
            "\tL2: 0.01\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (key_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (value_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (out_projection): Linear(in_features=1600, out_features=50, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (key_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (value_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (out_projection): Linear(in_features=1600, out_features=50, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: None\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: -1.0\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 50\n",
            "\t\tFF Dim: 100\n",
            "\t\tEnc Layers: 3\n",
            "\t\tDec Layers: 3\n",
            "\t\tEmbed Dropout: 0.1\n",
            "\t\tFF Dropout: 0.2\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.01\n",
            "\t\tWarmup Steps: 0\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='dp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='dp')` instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
            "  rank_zero_warn(\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 3.0 M \n",
            "----------------------------------------------------\n",
            "3.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.0 M     Total params\n",
            "12.084    Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:01<00:00,  1.19it/s]WARNING:root:Only 108 Image will be uploaded.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 0:  67% 1592/2376 [04:23<02:09,  6.05it/s, loss=0.00475, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  68% 1612/2376 [04:24<02:05,  6.09it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  69% 1632/2376 [04:26<02:01,  6.13it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  70% 1652/2376 [04:27<01:57,  6.18it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  70% 1672/2376 [04:28<01:53,  6.22it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  71% 1692/2376 [04:30<01:49,  6.27it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  72% 1712/2376 [04:31<01:45,  6.31it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  73% 1732/2376 [04:32<01:41,  6.35it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  74% 1752/2376 [04:33<01:37,  6.40it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  75% 1772/2376 [04:35<01:33,  6.44it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  75% 1792/2376 [04:36<01:30,  6.48it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  76% 1812/2376 [04:37<01:26,  6.52it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  77% 1832/2376 [04:39<01:22,  6.57it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  78% 1852/2376 [04:40<01:19,  6.61it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  79% 1872/2376 [04:41<01:15,  6.65it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  80% 1892/2376 [04:42<01:12,  6.69it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  80% 1912/2376 [04:44<01:08,  6.73it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  81% 1932/2376 [04:45<01:05,  6.77it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  82% 1952/2376 [04:46<01:02,  6.81it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  83% 1972/2376 [04:48<00:59,  6.84it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  84% 1992/2376 [04:49<00:55,  6.88it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  85% 2012/2376 [04:50<00:52,  6.92it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  86% 2032/2376 [04:51<00:49,  6.96it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  86% 2052/2376 [04:53<00:46,  7.00it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  87% 2072/2376 [04:54<00:43,  7.04it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  88% 2092/2376 [04:55<00:40,  7.07it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  89% 2112/2376 [04:57<00:37,  7.11it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  90% 2132/2376 [04:58<00:34,  7.14it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  91% 2152/2376 [04:59<00:31,  7.18it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  91% 2172/2376 [05:00<00:28,  7.22it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  92% 2192/2376 [05:02<00:25,  7.25it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  93% 2212/2376 [05:03<00:22,  7.29it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  94% 2232/2376 [05:04<00:19,  7.32it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  95% 2252/2376 [05:06<00:16,  7.36it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  96% 2272/2376 [05:07<00:14,  7.39it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  96% 2292/2376 [05:08<00:11,  7.42it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  97% 2312/2376 [05:10<00:08,  7.46it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  98% 2332/2376 [05:11<00:05,  7.49it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0:  99% 2352/2376 [05:12<00:03,  7.52it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0: 100% 2372/2376 [05:13<00:00,  7.56it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 0: 100% 2376/2376 [05:14<00:00,  7.56it/s, loss=0.00475, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 0: 100% 2376/2376 [06:50<00:00,  5.78it/s, loss=0.00475, v_num=uf56]\n",
            "Epoch 1: 100% 2376/2376 [06:51<00:00,  5.78it/s, loss=0.00475, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 1:  67% 1592/2376 [11:20<05:35,  2.34it/s, loss=0.00295, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  68% 1612/2376 [11:22<05:23,  2.36it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  69% 1632/2376 [11:23<05:11,  2.39it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  70% 1652/2376 [11:24<05:00,  2.41it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  70% 1672/2376 [11:26<04:48,  2.44it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  71% 1692/2376 [11:27<04:37,  2.46it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  72% 1712/2376 [11:28<04:27,  2.49it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  73% 1732/2376 [11:30<04:16,  2.51it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  74% 1752/2376 [11:31<04:06,  2.53it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  75% 1772/2376 [11:32<03:56,  2.56it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  75% 1792/2376 [11:34<03:46,  2.58it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  76% 1812/2376 [11:35<03:36,  2.61it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  77% 1832/2376 [11:36<03:26,  2.63it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  78% 1852/2376 [11:37<03:17,  2.65it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  79% 1872/2376 [11:39<03:08,  2.68it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  80% 1892/2376 [11:40<02:59,  2.70it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  80% 1912/2376 [11:41<02:50,  2.72it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  81% 1932/2376 [11:43<02:41,  2.75it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  82% 1952/2376 [11:44<02:33,  2.77it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  83% 1972/2376 [11:45<02:24,  2.79it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  84% 1992/2376 [11:47<02:16,  2.82it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  85% 2012/2376 [11:48<02:08,  2.84it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  86% 2032/2376 [11:49<02:00,  2.86it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  86% 2052/2376 [11:50<01:52,  2.89it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  87% 2072/2376 [11:52<01:44,  2.91it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  88% 2092/2376 [11:53<01:36,  2.93it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  89% 2112/2376 [11:54<01:29,  2.95it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  90% 2132/2376 [11:56<01:21,  2.98it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  91% 2152/2376 [11:57<01:14,  3.00it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  91% 2172/2376 [11:58<01:07,  3.02it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  92% 2192/2376 [11:59<01:00,  3.04it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  93% 2212/2376 [12:01<00:53,  3.07it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  94% 2232/2376 [12:02<00:46,  3.09it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  95% 2252/2376 [12:03<00:39,  3.11it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  96% 2272/2376 [12:05<00:33,  3.13it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  96% 2292/2376 [12:06<00:26,  3.16it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  97% 2312/2376 [12:07<00:20,  3.18it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  98% 2332/2376 [12:08<00:13,  3.20it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1:  99% 2352/2376 [12:10<00:07,  3.22it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1: 100% 2372/2376 [12:11<00:01,  3.24it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 1: 100% 2376/2376 [12:11<00:00,  3.25it/s, loss=0.00295, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 1: 100% 2376/2376 [13:48<00:00,  2.87it/s, loss=0.00295, v_num=uf56]\n",
            "Epoch 2: 100% 2376/2376 [13:48<00:00,  2.87it/s, loss=0.00295, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 2:  67% 1592/2376 [18:17<09:00,  1.45it/s, loss=0.0036, v_num=uf56] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  68% 1612/2376 [18:18<08:40,  1.47it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  69% 1632/2376 [18:20<08:21,  1.48it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  70% 1652/2376 [18:21<08:02,  1.50it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  70% 1672/2376 [18:22<07:44,  1.52it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  71% 1692/2376 [18:24<07:26,  1.53it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  72% 1712/2376 [18:25<07:08,  1.55it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  73% 1732/2376 [18:26<06:51,  1.57it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  74% 1752/2376 [18:27<06:34,  1.58it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  75% 1772/2376 [18:29<06:18,  1.60it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  75% 1792/2376 [18:30<06:01,  1.61it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  76% 1812/2376 [18:31<05:46,  1.63it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  77% 1832/2376 [18:33<05:30,  1.65it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  78% 1852/2376 [18:34<05:15,  1.66it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  79% 1872/2376 [18:35<05:00,  1.68it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  80% 1892/2376 [18:37<04:45,  1.69it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  80% 1912/2376 [18:38<04:31,  1.71it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  81% 1932/2376 [18:39<04:17,  1.73it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  82% 1952/2376 [18:40<04:03,  1.74it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  83% 1972/2376 [18:42<03:49,  1.76it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  84% 1992/2376 [18:43<03:36,  1.77it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  85% 2012/2376 [18:44<03:23,  1.79it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  86% 2032/2376 [18:46<03:10,  1.80it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  86% 2052/2376 [18:47<02:58,  1.82it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  87% 2072/2376 [18:48<02:45,  1.84it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  88% 2092/2376 [18:50<02:33,  1.85it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  89% 2112/2376 [18:51<02:21,  1.87it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  90% 2132/2376 [18:52<02:09,  1.88it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  91% 2152/2376 [18:53<01:58,  1.90it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  91% 2172/2376 [18:55<01:46,  1.91it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  92% 2192/2376 [18:56<01:35,  1.93it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  93% 2212/2376 [18:57<01:24,  1.94it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  94% 2232/2376 [18:59<01:13,  1.96it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  95% 2252/2376 [19:00<01:02,  1.97it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  96% 2272/2376 [19:01<00:52,  1.99it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  96% 2292/2376 [19:02<00:41,  2.01it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  97% 2312/2376 [19:04<00:31,  2.02it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  98% 2332/2376 [19:05<00:21,  2.04it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2:  99% 2352/2376 [19:06<00:11,  2.05it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2: 100% 2372/2376 [19:08<00:01,  2.07it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 2: 100% 2376/2376 [19:08<00:00,  2.07it/s, loss=0.0036, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 2: 100% 2376/2376 [20:45<00:00,  1.91it/s, loss=0.0036, v_num=uf56]\n",
            "Epoch 3: 100% 2376/2376 [20:45<00:00,  1.91it/s, loss=0.0036, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 3:  67% 1592/2376 [25:13<12:25,  1.05it/s, loss=0.00342, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  68% 1612/2376 [25:15<11:58,  1.06it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  69% 1632/2376 [25:16<11:31,  1.08it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  70% 1652/2376 [25:18<11:05,  1.09it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  70% 1672/2376 [25:19<10:39,  1.10it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  71% 1692/2376 [25:20<10:14,  1.11it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  72% 1712/2376 [25:22<09:50,  1.12it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  73% 1732/2376 [25:23<09:26,  1.14it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  74% 1752/2376 [25:24<09:03,  1.15it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  75% 1772/2376 [25:26<08:40,  1.16it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  75% 1792/2376 [25:27<08:17,  1.17it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  76% 1812/2376 [25:28<07:55,  1.19it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  77% 1832/2376 [25:29<07:34,  1.20it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  78% 1852/2376 [25:31<07:13,  1.21it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  79% 1872/2376 [25:32<06:52,  1.22it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  80% 1892/2376 [25:33<06:32,  1.23it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  80% 1912/2376 [25:35<06:12,  1.25it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  81% 1932/2376 [25:36<05:53,  1.26it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  82% 1952/2376 [25:37<05:33,  1.27it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  83% 1972/2376 [25:38<05:15,  1.28it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  84% 1992/2376 [25:40<04:56,  1.29it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  85% 2012/2376 [25:41<04:38,  1.31it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  86% 2032/2376 [25:42<04:21,  1.32it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  86% 2052/2376 [25:44<04:03,  1.33it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  87% 2072/2376 [25:45<03:46,  1.34it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  88% 2092/2376 [25:46<03:29,  1.35it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  89% 2112/2376 [25:47<03:13,  1.36it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  90% 2132/2376 [25:49<02:57,  1.38it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  91% 2152/2376 [25:50<02:41,  1.39it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  91% 2172/2376 [25:51<02:25,  1.40it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  92% 2192/2376 [25:53<02:10,  1.41it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  93% 2212/2376 [25:54<01:55,  1.42it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  94% 2232/2376 [25:55<01:40,  1.43it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  95% 2252/2376 [25:57<01:25,  1.45it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  96% 2272/2376 [25:58<01:11,  1.46it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  96% 2292/2376 [25:59<00:57,  1.47it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  97% 2312/2376 [26:00<00:43,  1.48it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  98% 2332/2376 [26:02<00:29,  1.49it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3:  99% 2352/2376 [26:03<00:15,  1.50it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3: 100% 2372/2376 [26:04<00:02,  1.52it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 3: 100% 2376/2376 [26:05<00:00,  1.52it/s, loss=0.00342, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 3: 100% 2376/2376 [27:42<00:00,  1.43it/s, loss=0.00342, v_num=uf56]\n",
            "Epoch 4: 100% 2376/2376 [27:42<00:00,  1.43it/s, loss=0.00342, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 4:  67% 1592/2376 [32:11<15:51,  1.21s/it, loss=0.00374, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  68% 1612/2376 [32:13<15:16,  1.20s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  69% 1632/2376 [32:14<14:41,  1.19s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  70% 1652/2376 [32:15<14:08,  1.17s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  70% 1672/2376 [32:17<13:35,  1.16s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  71% 1692/2376 [32:18<13:03,  1.15s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  72% 1712/2376 [32:19<12:32,  1.13s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  73% 1732/2376 [32:20<12:01,  1.12s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  74% 1752/2376 [32:22<11:31,  1.11s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  75% 1772/2376 [32:23<11:02,  1.10s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  75% 1792/2376 [32:24<10:33,  1.09s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  76% 1812/2376 [32:26<10:05,  1.07s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  77% 1832/2376 [32:27<09:38,  1.06s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  78% 1852/2376 [32:28<09:11,  1.05s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  79% 1872/2376 [32:29<08:44,  1.04s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  80% 1892/2376 [32:31<08:19,  1.03s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  80% 1912/2376 [32:32<07:53,  1.02s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  81% 1932/2376 [32:33<07:29,  1.01s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  82% 1952/2376 [32:35<07:04,  1.00s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  83% 1972/2376 [32:36<06:40,  1.01it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  84% 1992/2376 [32:37<06:17,  1.02it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  85% 2012/2376 [32:39<05:54,  1.03it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  86% 2032/2376 [32:40<05:31,  1.04it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  86% 2052/2376 [32:41<05:09,  1.05it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  87% 2072/2376 [32:42<04:48,  1.06it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  88% 2092/2376 [32:44<04:26,  1.07it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  89% 2112/2376 [32:45<04:05,  1.07it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  90% 2132/2376 [32:46<03:45,  1.08it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  91% 2152/2376 [32:48<03:24,  1.09it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  91% 2172/2376 [32:49<03:04,  1.10it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  92% 2192/2376 [32:50<02:45,  1.11it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  93% 2212/2376 [32:51<02:26,  1.12it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  94% 2232/2376 [32:53<02:07,  1.13it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  95% 2252/2376 [32:54<01:48,  1.14it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  96% 2272/2376 [32:55<01:30,  1.15it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  96% 2292/2376 [32:57<01:12,  1.16it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  97% 2312/2376 [32:58<00:54,  1.17it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  98% 2332/2376 [32:59<00:37,  1.18it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4:  99% 2352/2376 [33:01<00:20,  1.19it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4: 100% 2372/2376 [33:02<00:03,  1.20it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 4: 100% 2376/2376 [33:02<00:00,  1.20it/s, loss=0.00374, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 4: 100% 2376/2376 [34:39<00:00,  1.14it/s, loss=0.00374, v_num=uf56]\n",
            "Epoch 5: 100% 2376/2376 [34:39<00:00,  1.14it/s, loss=0.00374, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 5:  67% 1592/2376 [39:07<19:15,  1.47s/it, loss=0.00352, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  68% 1612/2376 [39:09<18:33,  1.46s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  69% 1632/2376 [39:10<17:51,  1.44s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  70% 1652/2376 [39:11<17:10,  1.42s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  70% 1672/2376 [39:13<16:30,  1.41s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  71% 1692/2376 [39:14<15:51,  1.39s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  72% 1712/2376 [39:15<15:13,  1.38s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  73% 1732/2376 [39:16<14:36,  1.36s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  74% 1752/2376 [39:18<13:59,  1.35s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  75% 1772/2376 [39:19<13:24,  1.33s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  75% 1792/2376 [39:20<12:49,  1.32s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  76% 1812/2376 [39:22<12:15,  1.30s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  77% 1832/2376 [39:23<11:41,  1.29s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  78% 1852/2376 [39:24<11:09,  1.28s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  79% 1872/2376 [39:26<10:37,  1.26s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  80% 1892/2376 [39:27<10:05,  1.25s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  80% 1912/2376 [39:28<09:34,  1.24s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  81% 1932/2376 [39:29<09:04,  1.23s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  82% 1952/2376 [39:31<08:35,  1.21s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  83% 1972/2376 [39:32<08:06,  1.20s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  84% 1992/2376 [39:33<07:37,  1.19s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  85% 2012/2376 [39:34<07:09,  1.18s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  86% 2032/2376 [39:36<06:42,  1.17s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  86% 2052/2376 [39:37<06:15,  1.16s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  87% 2072/2376 [39:38<05:49,  1.15s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  88% 2092/2376 [39:40<05:23,  1.14s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  89% 2112/2376 [39:41<04:57,  1.13s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  90% 2132/2376 [39:42<04:32,  1.12s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  91% 2152/2376 [39:44<04:08,  1.11s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  91% 2172/2376 [39:45<03:44,  1.10s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  92% 2192/2376 [39:46<03:20,  1.09s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  93% 2212/2376 [39:47<02:57,  1.08s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  94% 2232/2376 [39:49<02:34,  1.07s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  95% 2252/2376 [39:50<02:11,  1.06s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  96% 2272/2376 [39:51<01:49,  1.05s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  96% 2292/2376 [39:53<01:27,  1.04s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  97% 2312/2376 [39:54<01:06,  1.04s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  98% 2332/2376 [39:55<00:45,  1.03s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5:  99% 2352/2376 [39:56<00:24,  1.02s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5: 100% 2372/2376 [39:58<00:04,  1.01s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 5: 100% 2376/2376 [39:58<00:00,  1.01s/it, loss=0.00352, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 5: 100% 2376/2376 [41:35<00:00,  1.05s/it, loss=0.00352, v_num=uf56]\n",
            "Epoch 6: 100% 2376/2376 [41:35<00:00,  1.05s/it, loss=0.00352, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 6:  67% 1592/2376 [46:04<22:41,  1.74s/it, loss=0.00322, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  68% 1612/2376 [46:06<21:51,  1.72s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  69% 1632/2376 [46:07<21:01,  1.70s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  70% 1652/2376 [46:09<20:13,  1.68s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  70% 1672/2376 [46:10<19:26,  1.66s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  71% 1692/2376 [46:11<18:40,  1.64s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  72% 1712/2376 [46:12<17:55,  1.62s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  73% 1732/2376 [46:14<17:11,  1.60s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  74% 1752/2376 [46:15<16:28,  1.58s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  75% 1772/2376 [46:16<15:46,  1.57s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  75% 1792/2376 [46:18<15:05,  1.55s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  76% 1812/2376 [46:19<14:25,  1.53s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  77% 1832/2376 [46:20<13:45,  1.52s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  78% 1852/2376 [46:21<13:07,  1.50s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  79% 1872/2376 [46:23<12:29,  1.49s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  80% 1892/2376 [46:24<11:52,  1.47s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  80% 1912/2376 [46:25<11:16,  1.46s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  81% 1932/2376 [46:27<10:40,  1.44s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  82% 1952/2376 [46:28<10:05,  1.43s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  83% 1972/2376 [46:29<09:31,  1.41s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  84% 1992/2376 [46:31<08:58,  1.40s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  85% 2012/2376 [46:32<08:25,  1.39s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  86% 2032/2376 [46:33<07:52,  1.37s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  86% 2052/2376 [46:34<07:21,  1.36s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  87% 2072/2376 [46:36<06:50,  1.35s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  88% 2092/2376 [46:37<06:19,  1.34s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  89% 2112/2376 [46:38<05:49,  1.33s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  90% 2132/2376 [46:40<05:20,  1.31s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  91% 2152/2376 [46:41<04:51,  1.30s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  91% 2172/2376 [46:42<04:23,  1.29s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  92% 2192/2376 [46:44<03:55,  1.28s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  93% 2212/2376 [46:45<03:27,  1.27s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  94% 2232/2376 [46:46<03:01,  1.26s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  95% 2252/2376 [46:47<02:34,  1.25s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  96% 2272/2376 [46:49<02:08,  1.24s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  96% 2292/2376 [46:50<01:42,  1.23s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  97% 2312/2376 [46:51<01:17,  1.22s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  98% 2332/2376 [46:53<00:53,  1.21s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6:  99% 2352/2376 [46:54<00:28,  1.20s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6: 100% 2372/2376 [46:55<00:04,  1.19s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 6: 100% 2376/2376 [46:55<00:00,  1.19s/it, loss=0.00322, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 6: 100% 2376/2376 [48:31<00:00,  1.23s/it, loss=0.00322, v_num=uf56]\n",
            "Epoch 7: 100% 2376/2376 [48:31<00:00,  1.23s/it, loss=0.00322, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 7:  67% 1592/2376 [53:00<26:06,  2.00s/it, loss=0.00328, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  68% 1612/2376 [53:01<25:08,  1.97s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  69% 1632/2376 [53:03<24:11,  1.95s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  70% 1652/2376 [53:04<23:15,  1.93s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  70% 1672/2376 [53:05<22:21,  1.91s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  71% 1692/2376 [53:07<21:28,  1.88s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  72% 1712/2376 [53:08<20:36,  1.86s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  73% 1732/2376 [53:09<19:46,  1.84s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  74% 1752/2376 [53:11<18:56,  1.82s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  75% 1772/2376 [53:12<18:08,  1.80s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  75% 1792/2376 [53:13<17:20,  1.78s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  76% 1812/2376 [53:14<16:34,  1.76s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  77% 1832/2376 [53:16<15:49,  1.74s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  78% 1852/2376 [53:17<15:04,  1.73s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  79% 1872/2376 [53:18<14:21,  1.71s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  80% 1892/2376 [53:20<13:38,  1.69s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  80% 1912/2376 [53:21<12:56,  1.67s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  81% 1932/2376 [53:22<12:16,  1.66s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  82% 1952/2376 [53:23<11:35,  1.64s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  83% 1972/2376 [53:25<10:56,  1.63s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  84% 1992/2376 [53:26<10:18,  1.61s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  85% 2012/2376 [53:27<09:40,  1.59s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  86% 2032/2376 [53:29<09:03,  1.58s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  86% 2052/2376 [53:30<08:26,  1.56s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  87% 2072/2376 [53:31<07:51,  1.55s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  88% 2092/2376 [53:33<07:16,  1.54s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  89% 2112/2376 [53:34<06:41,  1.52s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  90% 2132/2376 [53:35<06:08,  1.51s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  91% 2152/2376 [53:36<05:34,  1.49s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  91% 2172/2376 [53:38<05:02,  1.48s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  92% 2192/2376 [53:39<04:30,  1.47s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  93% 2212/2376 [53:40<03:58,  1.46s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  94% 2232/2376 [53:42<03:27,  1.44s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  95% 2252/2376 [53:43<02:57,  1.43s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  96% 2272/2376 [53:44<02:27,  1.42s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  96% 2292/2376 [53:45<01:58,  1.41s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  97% 2312/2376 [53:47<01:29,  1.40s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  98% 2332/2376 [53:48<01:00,  1.38s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7:  99% 2352/2376 [53:49<00:32,  1.37s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7: 100% 2372/2376 [53:51<00:05,  1.36s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 7: 100% 2376/2376 [53:51<00:00,  1.36s/it, loss=0.00328, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 7: 100% 2376/2376 [55:26<00:00,  1.40s/it, loss=0.00328, v_num=uf56]\n",
            "Epoch 8: 100% 2376/2376 [55:26<00:00,  1.40s/it, loss=0.00328, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 8:  67% 1592/2376 [59:55<29:30,  2.26s/it, loss=0.00384, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  68% 1612/2376 [59:56<28:24,  2.23s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  69% 1632/2376 [59:58<27:20,  2.20s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  70% 1652/2376 [59:59<26:17,  2.18s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  70% 1672/2376 [1:00:00<25:16,  2.15s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  71% 1692/2376 [1:00:01<24:16,  2.13s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  72% 1712/2376 [1:00:03<23:17,  2.10s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  73% 1732/2376 [1:00:04<22:20,  2.08s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  74% 1752/2376 [1:00:05<21:24,  2.06s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  75% 1772/2376 [1:00:07<20:29,  2.04s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  75% 1792/2376 [1:00:08<19:35,  2.01s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  76% 1812/2376 [1:00:09<18:43,  1.99s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  77% 1832/2376 [1:00:11<17:52,  1.97s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  78% 1852/2376 [1:00:12<17:02,  1.95s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  79% 1872/2376 [1:00:13<16:12,  1.93s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  80% 1892/2376 [1:00:14<15:24,  1.91s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  80% 1912/2376 [1:00:16<14:37,  1.89s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  81% 1932/2376 [1:00:17<13:51,  1.87s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  82% 1952/2376 [1:00:18<13:06,  1.85s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  83% 1972/2376 [1:00:20<12:21,  1.84s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  84% 1992/2376 [1:00:21<11:38,  1.82s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  85% 2012/2376 [1:00:22<10:55,  1.80s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  86% 2032/2376 [1:00:23<10:13,  1.78s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  86% 2052/2376 [1:00:25<09:32,  1.77s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  87% 2072/2376 [1:00:26<08:52,  1.75s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  88% 2092/2376 [1:00:27<08:12,  1.73s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  89% 2112/2376 [1:00:29<07:33,  1.72s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  90% 2132/2376 [1:00:30<06:55,  1.70s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  91% 2152/2376 [1:00:31<06:18,  1.69s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  91% 2172/2376 [1:00:32<05:41,  1.67s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  92% 2192/2376 [1:00:34<05:05,  1.66s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  93% 2212/2376 [1:00:35<04:29,  1.64s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  94% 2232/2376 [1:00:36<03:54,  1.63s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  95% 2252/2376 [1:00:38<03:20,  1.62s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  96% 2272/2376 [1:00:39<02:46,  1.60s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  96% 2292/2376 [1:00:40<02:13,  1.59s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  97% 2312/2376 [1:00:41<01:40,  1.58s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  98% 2332/2376 [1:00:43<01:08,  1.56s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8:  99% 2352/2376 [1:00:44<00:37,  1.55s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8: 100% 2372/2376 [1:00:46<00:06,  1.54s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 8: 100% 2376/2376 [1:00:46<00:00,  1.53s/it, loss=0.00384, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 8: 100% 2376/2376 [1:02:24<00:00,  1.58s/it, loss=0.00384, v_num=uf56]\n",
            "Epoch 9: 100% 2376/2376 [1:02:24<00:00,  1.58s/it, loss=0.00384, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 9:  67% 1592/2376 [1:06:53<32:56,  2.52s/it, loss=0.00333, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  68% 1612/2376 [1:06:54<31:42,  2.49s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  69% 1632/2376 [1:06:56<30:30,  2.46s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  70% 1652/2376 [1:06:57<29:20,  2.43s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  70% 1672/2376 [1:06:58<28:12,  2.40s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  71% 1692/2376 [1:06:59<27:05,  2.38s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  72% 1712/2376 [1:07:01<25:59,  2.35s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  73% 1732/2376 [1:07:02<24:55,  2.32s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  74% 1752/2376 [1:07:03<23:53,  2.30s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  75% 1772/2376 [1:07:05<22:52,  2.27s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  75% 1792/2376 [1:07:06<21:52,  2.25s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  76% 1812/2376 [1:07:07<20:53,  2.22s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  77% 1832/2376 [1:07:09<19:56,  2.20s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  78% 1852/2376 [1:07:10<19:00,  2.18s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  79% 1872/2376 [1:07:11<18:05,  2.15s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  80% 1892/2376 [1:07:12<17:11,  2.13s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  80% 1912/2376 [1:07:14<16:19,  2.11s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  81% 1932/2376 [1:07:15<15:27,  2.09s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  82% 1952/2376 [1:07:16<14:36,  2.07s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  83% 1972/2376 [1:07:17<13:47,  2.05s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  84% 1992/2376 [1:07:19<12:58,  2.03s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  85% 2012/2376 [1:07:20<12:11,  2.01s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  86% 2032/2376 [1:07:21<11:24,  1.99s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  86% 2052/2376 [1:07:23<10:38,  1.97s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  87% 2072/2376 [1:07:24<09:53,  1.95s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  88% 2092/2376 [1:07:25<09:09,  1.93s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  89% 2112/2376 [1:07:27<08:25,  1.92s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  90% 2132/2376 [1:07:28<07:43,  1.90s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  91% 2152/2376 [1:07:29<07:01,  1.88s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  91% 2172/2376 [1:07:30<06:20,  1.87s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  92% 2192/2376 [1:07:32<05:40,  1.85s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  93% 2212/2376 [1:07:33<05:00,  1.83s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  94% 2232/2376 [1:07:34<04:21,  1.82s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  95% 2252/2376 [1:07:36<03:43,  1.80s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  96% 2272/2376 [1:07:37<03:05,  1.79s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  96% 2292/2376 [1:07:38<02:28,  1.77s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  97% 2312/2376 [1:07:39<01:52,  1.76s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  98% 2332/2376 [1:07:41<01:16,  1.74s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9:  99% 2352/2376 [1:07:42<00:41,  1.73s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9: 100% 2372/2376 [1:07:43<00:06,  1.71s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 9: 100% 2376/2376 [1:07:43<00:00,  1.71s/it, loss=0.00333, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 9: 100% 2376/2376 [1:09:20<00:00,  1.75s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10: 100% 2376/2376 [1:09:20<00:00,  1.75s/it, loss=0.00333, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 10:  67% 1592/2376 [1:13:47<36:20,  2.78s/it, loss=0.00333, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  68% 1612/2376 [1:13:49<34:59,  2.75s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  69% 1632/2376 [1:13:50<33:39,  2.71s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  70% 1652/2376 [1:13:52<32:22,  2.68s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  70% 1672/2376 [1:13:53<31:06,  2.65s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  71% 1692/2376 [1:13:54<29:52,  2.62s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  72% 1712/2376 [1:13:55<28:40,  2.59s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  73% 1732/2376 [1:13:57<27:29,  2.56s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  74% 1752/2376 [1:13:58<26:20,  2.53s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  75% 1772/2376 [1:13:59<25:13,  2.51s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  75% 1792/2376 [1:14:00<24:07,  2.48s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  76% 1812/2376 [1:14:02<23:02,  2.45s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  77% 1832/2376 [1:14:03<21:59,  2.43s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  78% 1852/2376 [1:14:04<20:57,  2.40s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  79% 1872/2376 [1:14:06<19:57,  2.38s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  80% 1892/2376 [1:14:07<18:57,  2.35s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  80% 1912/2376 [1:14:08<17:59,  2.33s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  81% 1932/2376 [1:14:10<17:02,  2.30s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  82% 1952/2376 [1:14:11<16:06,  2.28s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  83% 1972/2376 [1:14:12<15:12,  2.26s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  84% 1992/2376 [1:14:13<14:18,  2.24s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  85% 2012/2376 [1:14:15<13:25,  2.21s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  86% 2032/2376 [1:14:16<12:34,  2.19s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  86% 2052/2376 [1:14:17<11:43,  2.17s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  87% 2072/2376 [1:14:18<10:54,  2.15s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  88% 2092/2376 [1:14:20<10:05,  2.13s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  89% 2112/2376 [1:14:21<09:17,  2.11s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  90% 2132/2376 [1:14:22<08:30,  2.09s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  91% 2152/2376 [1:14:24<07:44,  2.07s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  91% 2172/2376 [1:14:25<06:59,  2.06s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  92% 2192/2376 [1:14:26<06:14,  2.04s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  93% 2212/2376 [1:14:27<05:31,  2.02s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  94% 2232/2376 [1:14:29<04:48,  2.00s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  95% 2252/2376 [1:14:30<04:06,  1.99s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  96% 2272/2376 [1:14:31<03:24,  1.97s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  96% 2292/2376 [1:14:33<02:43,  1.95s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  97% 2312/2376 [1:14:34<02:03,  1.94s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  98% 2332/2376 [1:14:35<01:24,  1.92s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10:  99% 2352/2376 [1:14:36<00:45,  1.90s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10: 100% 2372/2376 [1:14:38<00:07,  1.89s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 10: 100% 2376/2376 [1:14:38<00:00,  1.88s/it, loss=0.00333, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 10: 100% 2376/2376 [1:16:12<00:00,  1.92s/it, loss=0.00333, v_num=uf56]\n",
            "Epoch 11: 100% 2376/2376 [1:16:12<00:00,  1.92s/it, loss=0.00333, v_num=uf56]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 11:  67% 1592/2376 [1:20:40<39:43,  3.04s/it, loss=0.00374, v_num=uf56]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/784 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  68% 1612/2376 [1:20:41<38:14,  3.00s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  69% 1632/2376 [1:20:43<36:47,  2.97s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  70% 1652/2376 [1:20:44<35:23,  2.93s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  70% 1672/2376 [1:20:45<34:00,  2.90s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  71% 1692/2376 [1:20:47<32:39,  2.86s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  72% 1712/2376 [1:20:48<31:20,  2.83s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  73% 1732/2376 [1:20:49<30:03,  2.80s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  74% 1752/2376 [1:20:50<28:47,  2.77s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  75% 1772/2376 [1:20:52<27:33,  2.74s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  75% 1792/2376 [1:20:53<26:21,  2.71s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  76% 1812/2376 [1:20:54<25:11,  2.68s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  77% 1832/2376 [1:20:56<24:01,  2.65s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  78% 1852/2376 [1:20:57<22:54,  2.62s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  79% 1872/2376 [1:20:58<21:48,  2.60s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  80% 1892/2376 [1:20:59<20:43,  2.57s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  80% 1912/2376 [1:21:01<19:39,  2.54s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  81% 1932/2376 [1:21:02<18:37,  2.52s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  82% 1952/2376 [1:21:03<17:36,  2.49s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  83% 1972/2376 [1:21:05<16:36,  2.47s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  84% 1992/2376 [1:21:06<15:38,  2.44s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  85% 2012/2376 [1:21:07<14:40,  2.42s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  86% 2032/2376 [1:21:08<13:44,  2.40s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  86% 2052/2376 [1:21:10<12:48,  2.37s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  87% 2072/2376 [1:21:11<11:54,  2.35s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  88% 2092/2376 [1:21:12<11:01,  2.33s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  89% 2112/2376 [1:21:14<10:09,  2.31s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  90% 2132/2376 [1:21:15<09:17,  2.29s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  91% 2152/2376 [1:21:16<08:27,  2.27s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  91% 2172/2376 [1:21:17<07:38,  2.25s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  92% 2192/2376 [1:21:19<06:49,  2.23s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  93% 2212/2376 [1:21:20<06:01,  2.21s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  94% 2232/2376 [1:21:21<05:14,  2.19s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  95% 2252/2376 [1:21:23<04:28,  2.17s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  96% 2272/2376 [1:21:24<03:43,  2.15s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  96% 2292/2376 [1:21:25<02:59,  2.13s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  97% 2312/2376 [1:21:26<02:15,  2.11s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  98% 2332/2376 [1:21:28<01:32,  2.10s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11:  99% 2352/2376 [1:21:29<00:49,  2.08s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11: 100% 2372/2376 [1:21:30<00:08,  2.06s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11: 100% 2376/2376 [1:21:31<00:00,  2.06s/it, loss=0.00374, v_num=uf56]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 11: 100% 2376/2376 [1:23:04<00:00,  2.10s/it, loss=0.00374, v_num=uf56]\n",
            "Epoch 11: 100% 2376/2376 [1:23:04<00:00,  2.10s/it, loss=0.00374, v_num=uf56]\n",
            "Restoring states from the checkpoint path at /content/spacetimeformer/data/STF_LOG_DIR/temporal_asos_160-40-nll_ff489108/temporal_asos_160-40-nllepoch=06.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at /content/spacetimeformer/data/STF_LOG_DIR/temporal_asos_160-40-nll_ff489108/temporal_asos_160-40-nllepoch=06.ckpt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Testing DataLoader 0: 100% 245/245 [00:16<00:00, 15.01it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          -1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m     test/class_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m   test/forecast_loss    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0034162034280598164  \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0034162034280598164  \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.029634078628266863   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test/mape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   115365.44245524297    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0034161643298995462  \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      test/norm_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.029634078628266863   \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      test/norm_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0034161643298995462  \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m     test/recon_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          -1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       test/smape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.6623570122682225    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ██████████▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test/class_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test/forecast_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/mae ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test/mape ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/mse ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/norm_mae ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/norm_mse ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test/recon_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test/smape ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss ▅▅▃▆▃▃▃▂▃▆▃▆▃▃▂▃▄▂▅▃▅▃▁▄▅▃▄▃▄▂█▄▅▃▃▃▃▅▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▅▅▃▆▃▃▃▂▃▆▃▆▃▃▂▃▄▂▅▃▅▃▁▄▅▃▄▃▄▂█▄▅▃▃▃▃▅▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae ▇▅▃█▃▃▃▂▃▆▃▅▃▃▁▃▅▂▅▂▅▂▁▃▅▂▄▂▄▂▆▄▆▃▃▃▂▆▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape █▅▄█▄▂▅▄▅▃▆▂▄▅▃▄▇▃▂▄▄▄▆▄▄▃▄▃▄▅▅▄▁▃▆▅▃▁▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse ▅▅▃▆▃▃▃▂▃▆▃▆▃▃▂▃▄▂▅▃▅▃▁▄▅▃▄▃▄▂█▄▅▃▃▃▃▅▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae ▇▅▃█▃▃▃▂▃▆▃▅▃▃▁▃▅▂▅▂▅▂▁▃▅▂▄▂▄▂▆▄▆▃▃▃▂▆▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse ▅▅▃▆▃▃▃▂▃▆▃▆▃▃▂▃▄▂▅▃▅▃▁▄▅▃▄▃▄▂█▄▅▃▃▃▃▅▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape █▇▅▆▅▄▆▅▇▄▇▃▆▅▅▆▆▅▃▆▄▆▇▅▅▅▄▅▄▇▆▄▁▅▇▇▅▂▆▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss ▅▃▃▅▂█▁▂█▃▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss ▅▃▃▅▂█▁▂█▃▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae ▅▂▂▂▃█▁▂▇▃▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▄▂▂▂▃█▁▂▇▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse ▅▃▃▅▂█▁▂█▃▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae ▅▂▂▂▃█▁▂▇▃▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse ▅▃▃▅▂█▁▂█▃▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape █▅▂▄▃▄▃▂▄▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step 19104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.00013\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/acc -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test/class_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test/forecast_loss 0.00342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test/loss 0.00342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/mae 0.02963\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test/mape 115365.44246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/mse 0.00342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/norm_mae 0.02963\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/norm_mse 0.00342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test/smape 1.66236\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.00345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.00345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.03019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 127744.22236\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.00345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.03019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.00345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.65373\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 19104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.00328\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.00328\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.03129\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 133689.87071\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.00328\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.03129\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.00328\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.64948\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdutiful-morning-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer/runs/6olxuf56\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1690 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./data/STF_LOG_DIR/wandb/run-20230522_125541-6olxuf56/logs\u001b[0m\n",
            "Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./data/STF_LOG_DIR/wandb/run-20230522_142113-rm9x9c3a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbrisk-wildflower-11\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer/runs/rm9x9c3a\u001b[0m\n",
            "Forecaster\n",
            "\tL2: 0.01\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (key_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (value_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (out_projection): Linear(in_features=1600, out_features=50, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (key_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (value_projection): Linear(in_features=50, out_features=1600, bias=True)\n",
            "  (out_projection): Linear(in_features=1600, out_features=50, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: None\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: -1.0\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 50\n",
            "\t\tFF Dim: 100\n",
            "\t\tEnc Layers: 3\n",
            "\t\tDec Layers: 3\n",
            "\t\tEmbed Dropout: 0.1\n",
            "\t\tFF Dropout: 0.2\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.01\n",
            "\t\tWarmup Steps: 0\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spacetimeformer/spacetimeformer/train.py\", line 869, in <module>\n",
            "    main(args)\n",
            "  File \"/content/spacetimeformer/spacetimeformer/train.py\", line 826, in main\n",
            "    config.update(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_config.py\", line 184, in update\n",
            "    sanitized = self._update(d, allow_val_change)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_config.py\", line 177, in _update\n",
            "    sanitized = self._sanitize_dict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_config.py\", line 237, in _sanitize_dict\n",
            "    k, v = self._sanitize(k, v, allow_val_change)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_config.py\", line 261, in _sanitize\n",
            "    raise config_util.ConfigError(\n",
            "wandb.sdk.lib.config_util.ConfigError: Attempted to change value of key \"model_ckpt_dir\" from ./data/STF_LOG_DIR/temporal_asos_160-40-nll_ff489108 to ./data/STF_LOG_DIR/temporal_asos_160-40-nll_f46854c4\n",
            "If you really want to do this, pass allow_val_change=True to config.update()\n"
          ]
        }
      ],
      "source": [
        "!python spacetimeformer/train.py spacetimeformer precip --dset_dir //content/spacetimeformer/spacetimeformer/data/dset_precip --context_points 160 --target_points 40 --start_token_len 8 --grad_clip_norm 1 --gpus 0 --batch_size 8 --d_model 50 --d_ff 100 --enc_layers 3 --dec_layers 3 --local_self_attn none --local_cross_attn none --l2_coeff .01 --dropout_emb .1 --run_name temporal_asos_160-40-nll --loss mse --dropout_ff .2 --n_heads 8 --trials 3 --embed_method temporal --wandb --attn_plot --plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpg4Osy1B7AZ",
        "outputId": "6d872d08-55fb-4a3b-b717-9ad2f8d986f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpooyakalahroodi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./data/STF_LOG_DIR/wandb/run-20230522_161314-ahlcn8os\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswift-sun-18\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/pooyakalahroodi/spacetimeformer/runs/ahlcn8os\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "Forecaster\n",
            "\tL2: 0.01\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (out_projection): Linear(in_features=400, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (out_projection): Linear(in_features=400, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (out_projection): Linear(in_features=400, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=400, bias=True)\n",
            "  (out_projection): Linear(in_features=400, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: -1.0\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 20\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.1\n",
            "\t\tFF Dropout: 0.2\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.01\n",
            "\t\tWarmup Steps: 0\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='dp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='dp')` instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
            "  rank_zero_warn(\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 314 K \n",
            "----------------------------------------------------\n",
            "314 K     Trainable params\n",
            "0         Non-trainable params\n",
            "314 K     Total params\n",
            "1.258     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:02<00:00,  1.23s/it]WARNING:root:Only 108 Image will be uploaded.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 0:  67% 3183/4750 [1:00:24<29:44,  1.14s/it, loss=0.16, v_num=n8os] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/1567 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  67% 3203/4750 [1:00:33<29:14,  1.13s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  68% 3223/4750 [1:00:41<28:45,  1.13s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  68% 3243/4750 [1:00:50<28:16,  1.13s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  69% 3263/4750 [1:00:58<27:47,  1.12s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  69% 3283/4750 [1:01:07<27:18,  1.12s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  70% 3303/4750 [1:01:15<26:50,  1.11s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  70% 3323/4750 [1:01:24<26:22,  1.11s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  70% 3343/4750 [1:01:32<25:54,  1.10s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  71% 3363/4750 [1:01:41<25:26,  1.10s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  71% 3383/4750 [1:01:49<24:59,  1.10s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  72% 3403/4750 [1:01:58<24:31,  1.09s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  72% 3423/4750 [1:02:06<24:04,  1.09s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  72% 3443/4750 [1:02:15<23:38,  1.08s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  73% 3463/4750 [1:02:23<23:11,  1.08s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  73% 3483/4750 [1:02:32<22:45,  1.08s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  74% 3503/4750 [1:02:40<22:18,  1.07s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  74% 3523/4750 [1:02:49<21:52,  1.07s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  75% 3543/4750 [1:02:57<21:27,  1.07s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  75% 3563/4750 [1:03:06<21:01,  1.06s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  75% 3583/4750 [1:03:14<20:36,  1.06s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  76% 3603/4750 [1:03:23<20:10,  1.06s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  76% 3623/4750 [1:03:31<19:45,  1.05s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  77% 3643/4750 [1:03:40<19:20,  1.05s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  77% 3663/4750 [1:03:48<18:56,  1.05s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  78% 3683/4750 [1:03:57<18:31,  1.04s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  78% 3703/4750 [1:04:05<18:07,  1.04s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  78% 3723/4750 [1:04:14<17:43,  1.04s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  79% 3743/4750 [1:04:22<17:19,  1.03s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  79% 3763/4750 [1:04:31<16:55,  1.03s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  80% 3783/4750 [1:04:39<16:31,  1.03s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  80% 3803/4750 [1:04:48<16:08,  1.02s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  80% 3823/4750 [1:04:56<15:44,  1.02s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  81% 3843/4750 [1:05:05<15:21,  1.02s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  81% 3863/4750 [1:05:13<14:58,  1.01s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  82% 3883/4750 [1:05:22<14:35,  1.01s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  82% 3903/4750 [1:05:30<14:13,  1.01s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  83% 3923/4750 [1:05:39<13:50,  1.00s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  83% 3943/4750 [1:05:47<13:27,  1.00s/it, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  83% 3963/4750 [1:05:56<13:05,  1.00it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  84% 3983/4750 [1:06:04<12:43,  1.00it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  84% 4003/4750 [1:06:13<12:21,  1.01it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  85% 4023/4750 [1:06:21<11:59,  1.01it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  85% 4043/4750 [1:06:30<11:37,  1.01it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  86% 4063/4750 [1:06:38<11:16,  1.02it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  86% 4083/4750 [1:06:47<10:54,  1.02it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  86% 4103/4750 [1:06:55<10:33,  1.02it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  87% 4123/4750 [1:07:04<10:11,  1.02it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  87% 4143/4750 [1:07:12<09:50,  1.03it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  88% 4163/4750 [1:07:21<09:29,  1.03it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  88% 4183/4750 [1:07:29<09:08,  1.03it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  88% 4203/4750 [1:07:38<08:48,  1.04it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  89% 4223/4750 [1:07:46<08:27,  1.04it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  89% 4243/4750 [1:07:54<08:06,  1.04it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  90% 4263/4750 [1:08:03<07:46,  1.04it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  90% 4283/4750 [1:08:11<07:26,  1.05it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  91% 4303/4750 [1:08:20<07:05,  1.05it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  91% 4323/4750 [1:08:28<06:45,  1.05it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  91% 4343/4750 [1:08:37<06:25,  1.05it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  92% 4363/4750 [1:08:45<06:05,  1.06it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  92% 4383/4750 [1:08:54<05:46,  1.06it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  93% 4403/4750 [1:09:02<05:26,  1.06it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  93% 4423/4750 [1:09:11<05:06,  1.07it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  94% 4443/4750 [1:09:19<04:47,  1.07it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  94% 4463/4750 [1:09:28<04:28,  1.07it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  94% 4483/4750 [1:09:36<04:08,  1.07it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  95% 4503/4750 [1:09:45<03:49,  1.08it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  95% 4523/4750 [1:09:53<03:30,  1.08it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  96% 4543/4750 [1:10:02<03:11,  1.08it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  96% 4563/4750 [1:10:10<02:52,  1.08it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  96% 4583/4750 [1:10:19<02:33,  1.09it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  97% 4603/4750 [1:10:27<02:15,  1.09it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  97% 4623/4750 [1:10:36<01:56,  1.09it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  98% 4643/4750 [1:10:44<01:37,  1.09it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  98% 4663/4750 [1:10:53<01:19,  1.10it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  99% 4683/4750 [1:11:01<01:00,  1.10it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  99% 4703/4750 [1:11:10<00:42,  1.10it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0:  99% 4723/4750 [1:11:18<00:24,  1.10it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0: 100% 4743/4750 [1:11:27<00:06,  1.11it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 0: 100% 4750/4750 [1:11:29<00:00,  1.11it/s, loss=0.16, v_num=n8os]WARNING:root:Only 108 Image will be uploaded.\n",
            "Epoch 0: 100% 4750/4750 [1:12:30<00:00,  1.09it/s, loss=0.16, v_num=n8os]\n",
            "Epoch 1: 100% 4750/4750 [1:12:30<00:00,  1.09it/s, loss=0.16, v_num=n8os]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 1:  18% 840/4750 [1:28:28<6:51:48,  6.32s/it, loss=0.147, v_num=n8os]"
          ]
        }
      ],
      "source": [
        "!python spacetimeformer/train.py spacetimeformer precip --dset_dir //content/spacetimeformer/spacetimeformer/data/dset_precip --context_points 160 --target_points 40 --start_token_len 8 --grad_clip_norm 1 --gpus 0 --batch_size 4 --d_model 15 --d_ff 20 --enc_layers 2 --dec_layers 2 --local_self_attn performer --local_cross_attn performer --l2_coeff .01 --dropout_emb .1 --run_name temporal_asos_160-40-nll --loss mse --dropout_ff .2 --n_heads 2 --trials 2 --embed_method spatio-temporal --wandb --attn_plot --plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vPgQZsPSec-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208d57ae-f880-4510-f8fe-21ec374bb26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 116, in _service_connect\n",
            "    svc_iface._svc_connect(port=port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/service/service_sock.py\", line 30, in _svc_connect\n",
            "    self._sock_client.connect(port=port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 102, in connect\n",
            "    s.connect((\"localhost\", port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spacetimeformer/spacetimeformer/train.py\", line 869, in <module>\n",
            "    main(args)\n",
            "  File \"/content/spacetimeformer/spacetimeformer/train.py\", line 749, in main\n",
            "    experiment = wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1169, in init\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1146, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 172, in setup\n",
            "    self._wl = wandb_setup.setup(settings=setup_settings)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 327, in setup\n",
            "    ret = _setup(settings=settings)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 320, in _setup\n",
            "    wl = _WandbSetup(settings=settings)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 303, in __init__\n",
            "    _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 114, in __init__\n",
            "    self._setup()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 250, in _setup\n",
            "    self._setup_manager()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_setup.py\", line 277, in _setup_manager\n",
            "    self._manager = wandb_manager._Manager(settings=self._settings)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 163, in __init__\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 146, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 161, in __init__\n",
            "    self._service_connect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 125, in _service_connect\n",
            "    raise ManagerConnectionRefusedError(message)\n",
            "wandb.sdk.wandb_manager.ManagerConnectionRefusedError: Connection to wandb service failed: [Errno 111] Connection refused. \n"
          ]
        }
      ],
      "source": [
        "!python spacetimeformer/train.py spacetimeformer precip --dset_dir //content/spacetimeformer/spacetimeformer/data/dset_precip --context_points 160 --target_points 40 --start_token_len 8 --grad_clip_norm 1 --gpus 0 --batch_size 8 --d_model 20 --d_ff 20 --enc_layers 3 --dec_layers 3 --local_self_attn none --local_cross_attn none --l2_coeff .01 --dropout_emb .1 --run_name temporal_asos_160-40-nll --workers 4 --loss mse --dropout_ff .2 --n_heads 4 --trials 3 --embed_method spatio-temporal --wandb --attn_plot --plot"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrcFoqHh72jS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPghMcoHfpoiqmTfNEAyuSj",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
